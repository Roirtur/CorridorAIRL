\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel} % Pour la typographie française
\usepackage{graphicx}
\usepackage{array}
\usepackage[table]{xcolor}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{float}
\usepackage{changepage}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{geometry} % Pour gérer les marges
\usepackage{enumitem}

\usetikzlibrary{trees}

\lstdefinestyle{treeStyle}{
    basicstyle=\small\ttfamily,
    breaklines=true,
    frame=single, % Ajoute un cadre
    frameround=tttt,
    rulesepcolor=\color{gray!50},
    captionpos=b,
    numbers=none
}

\geometry{hmargin=2.5cm,vmargin=2.5cm}

\begin{document}
\begin{titlepage}
    \newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
    
    \center 
    
    \textsc{\LARGE Université de Bordeaux}\\[1.5cm] 
    \textsc{\Large Master 2 Informatique - Intelligence Artificielle}\\[0.5cm] 
    \textsc{\large Apprentissage par Renforcement}\\[0.5cm]
    
    \vspace{1cm}
    \HRule \\[0.4cm]
    { \huge \bfseries Agent Intelligent pour le Jeu de Corridor}\\[0.4cm] 
    \HRule \\[1.5cm]
    
    \includegraphics[width=0.6\textwidth]{images/Quoridor_1.jpg}\\[1.5cm]
    
    \begin{minipage}{0.4\textwidth}
        \begin{flushleft} \large
        \emph{Auteurs :}\\
        Arthur \textsc{Macdonald}\\
        Niels \textsc{Roudeau}
        \end{flushleft}
    \end{minipage}
    ~
    \begin{minipage}{0.4\textwidth}
        \begin{flushright} \large
        \emph{Date :} \\
        2025 -- 2026
        \end{flushright}
    \end{minipage}\\[2cm]
    
    \vfill 
    
\end{titlepage}

\tableofcontents
\newpage


\section{Présentation du projet}
L'objectif de notre projet est d'implémenter un agent intelligent capable de jouer au jeu Corridor, une implémentaion simplifié du jeu Quoridor. 
Nous avons implémenté plusieurs agents, chacuns utilisant des méthodes d'apprentissages basées sur l'apprentissage par renforcement : 

\begin{itemize}
    \item \textbf{Q-Learning / Sarsa} (Apprentissage tabulaire)
    \item \textbf{Deep Q-Network} (Réseau de neurones)
\end{itemize}

Nos agents comprennent le jeu à l'aide d'un processus de décision Markovien (MDP) la définition et l'implémentation de ce processus font parti de ce rapport.

Nous traiterons dans un premier temps de nos choix d'implémentation pour ces agents.

Enfin, nous présenterons nos résultats expérimentaux obtenus lors de phases d'entrainement et d'évaluation de nos modèles.

\section{Architecture du Projet}
\lstset{style=treeStyle}
\begin{lstlisting}
.
|-- corridor.py
|-- corridor_starter.py 
|-- experiments.ipynb
|-- start_evaluation.py
|-- start_training.py   
|-- models/
|   |-- __init__.py
|   |-- base_agent.py
|   |-- dqn/
|        |-- __init__.py
|        |-- action_encoder.py
|        |-- prioritized_replay.py
|        |-- dqn_agent.py
|        |-- dqn_network.py
|   |-- greedy/
|        |-- greedy_path_agent.py
|   |-- qlearning/
|        |-- qlearning_agent.py
|   |-- sarsa/
|        |-- sarsa_agent.py
|   |-- random/ 
|        |-- random_agent.py
|-- saved_models/
|-- utils/
|   |-- representation.py
|   |-- saving.py 
|   |-- training.py
|-- projet.pdf
|-- report
|   |-- report.pdf
|   |-- report.tex
|-- requirements.txt 

\end{lstlisting}

Notre travail comprend:
\begin{itemize}
    \item Implémentation de deux agents intelligents suivant les méthodes d'apprentissage tabulaires Q-Learning et Sarsa.
    \item Implémentation d'un agent utilisant un Deep Q-Network.
    \item Définition d'un MDP autour du jeu de Corridor.
    \item \textbf{start\_evaluation.py,  start\_training.py}: programmes utilisables en CLI pour l'entrainement et l'évaluation des modèles.
    \item \textbf{experiments.ipynb}: Notebook Jupyter sur nos procédés experimentaux et leurs résultats 
\end{itemize}

\section{Définition d'un MDP}

\subsection{Représentation de l'état d'une partie pour Apprentissage tabulaire}

La difficulté principale dans l'apprentissage tabulaire du jeu Corridor est d'éviter une explosion combinatoire de l'espace d'états (\textit{Curse of Dimensionality}).

Pour pallier à cela, nous avons conçu une représentation compacte, visant à réduire la cardinalité de l'espace tout en conservant les informations stratégiques cruciales. Notre tuple d'état $(S)$ est construit selon les principes suivants:

\begin{itemize}
    \item \textbf{Binning des Murs :} Le nombre exact de murs restants modifie rarement la stratégie optimale. Nous avons donc discrétisé cette valeur en intervalles. L'agent regroupe des états stratégiquement équivalents et accélère donc la convergence de la Q-Table.
    
    \item \textbf{Abstraction des obstacles :} Plutôt que d'encoder la position de chaque mur, nous capturons l'impact topologique des murs en calculant la distance du plus court chemin à l'arrivée via BFS pour chaque joueur.
    
    \item \textbf{Encodage de la topologie locale :} Pour faciliter l'apprentissage des mouvements légaux sans avoir à parser la liste globale des murs, nous incluons un \textit{masque d'adjacence}. Il s'agit d'un encodage binaire représentant les obstacles immédiats (murs ou bords du plateau) autour des deux joueurs. Cela offre à l'agent une "vision locale" immédiate.
    
    \item \textbf{Symétrie de plateau:} Le jeu présentant une symétrie centrale, nous normalisons systématiquement l'observation. L'état est toujours représenté du point de vue d'un joueur devant atteindre le bas du plateau.
    
\end{itemize}

L'état final est donc un tuple : 
$$ S = (Pos_{me}, Pos_{opp}, Walls_{me}^{bin}, Dist_{me}, Dist_{opp}, Mask_{me}, Mask_{opp}) $$

Selon nos expérimentations et les documents scientifiques consultés durant nos travaux, le nombre de murs restants pour l'adversaire n'est pas un élément pertinent à utiliser. Il ne fait qu'accélerer l'explosion combinatoire.


Notre implémentaion a été dirigée dans l'intêret d'avoir des principes théoriquement "séparés". Par exemple l'ajout d'informations sur les murs feraient perdre sens à notre choix d'utiliser un binning, on se contente d'utiliser des informations simples qui empêchent l'agent de se perdre dans ses décisions.


\subsection{Représentation de l'état d'une partie pour Apprentissage profond}

Contrairement à l'approche tabulaire où nous avons dû extraire manuellement des caractéristiques de haut niveau (distances, masques), l'approche par Deep Q-Network (DQN) repose sur la capacité des réseaux de neurones convolutifs (CNN) à extraire eux-mêmes les motifs pertinents à partir de données brutes.

Nous avons donc opté pour une représentation qui préserve la structure spatiale du jeu. L'état n'est plus un tuple abstrait, mais un tenseur de forme $(C \times N \times N)$.

Notre représentation comporte $C=6$ canaux, normalisés entre 0 et 1 pour assurer la stabilité numérique du réseau :

\begin{itemize}
    \item \textbf{Canaux de Position (2 plans) :} Deux grilles binaires encodant respectivement la position du joueur courant ($P1$) et celle de l'adversaire ($P2$). Cela permet au réseau de situer les agents dans l'espace sans ambiguïté.
    
    \item \textbf{Canaux de Topologie (2 plans) :} Deux grilles binaires représentant les murs posés (un plan pour les murs horizontaux, un pour les verticaux). Contrairement à une liste de coordonnées, cette représentation matricielle permet aux filtres de convolution de détecter des motifs locaux comme des "couloirs", des "impasses" ou des "blocages".
    
    \item \textbf{Canaux de Contexte (2 plans) :} Le nombre de murs restants est une information scalaire, mais cruciale. Pour l'intégrer dans une architecture convolutionnelle, nous utilisons la technique des "plans scalaires" (utilisée notamment par AlphaGo) : nous remplissons deux plans entiers avec la valeur normalisée du nombre de murs restants (pour soi et pour l'adversaire). Cela fournit un biais constant à chaque neurone de la couche d'entrée, conditionnant l'analyse spatiale aux ressources disponibles.
\end{itemize}

\subsection{Système de Récompenses}

Conformément à l'implémentation du moteur, les récompenses terminales sont :
\[
r_T = 
\begin{cases}
+1.0 & \text{si l'agent gagne au coup terminal},\\[2pt]
-1.0 & \text{si l'agent perd},\\[2pt]
-0.5 & \text{en cas de match nul / timeout (pour certaines expériences).}
\end{cases}
\]

Pour les agents tabulaires (Q-Learning, SARSA) nous avons ajouté un terme de shaping basé sur la variation de la distance au but calculée par BFS. À chaque pas pertinent de l'agent, la récompense non terminale est :
\[
r_t = \alpha \cdot \big(d_{t-1} - d_t\big) + c_{\text{step}},
\]
avec les hyper-paramètres utilisés dans le code :
\begin{itemize}
    \item $\alpha = 0.1$ (gain sur la réduction de la distance) ;
    \item $c_{\text{step}} = -0.01$ (petite pénalité de pas pour décourager les aller-retour).
\end{itemize}
Ainsi, chaque action qui rapproche l'agent de son objectif reçoit une récompense positive proportionnelle à la diminution de la distance; les actions qui l'éloignent reçoivent une récompense négative relative. Ce shaping est appliqué uniquement pendant l'entraînement tabulaire afin d'accélérer la propagation des valeurs dans la Q-table.

\paragraph{Justification et limites}
Le shaping par différence de distance satisfait l'intuition stratégique (progresser vers la ligne adverse) et réduit la variance des retours en fournissant un signal dense. Néanmoins :
\begin{itemize}
    \item il peut modifier la politique optimale si mal calibré (risque d'introduire un biais vers des trajectoires ``courtes'' mais tactiquement faibles) ;
    \item il dépend de la métrique de distance choisie (ici shortest path via BFS) — lorsque les murs sont posés l'estimation peut varier brutalement ;
    \item il n'est appliqué qu'aux agents tabulaires : les agents DQN, traitant une représentation spatiale riche, reçoivent par défaut les récompenses fournies par l'environnement (0 pour coups non-terminaux, +1/-1 terminaux). Dans les expérimentations, on peut néanmoins introduire le même shaping pour DQN si nécessaire.
\end{itemize}


\section{Resultats expérimentaux}
L'ensemble de nos procédés et résultats experimentaux sont regroupés dans le fichier \textbf{experiments.ipynb}

\end{document}